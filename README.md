# Data Pipeline - Multi-Source Real-Time Data Integration

## üìã Quick Overview

**Data Pipeline** is a sophisticated **event-driven streaming data integration system** that fetches data from multiple e-commerce APIs (FakeStore and DummyJSON), processes it in real-time using Apache Kafka and Apache Spark, and persists normalized data into PostgreSQL for analytics and visualization.

**Problem Solved**: 
- ‚úÖ Integrate data from multiple heterogeneous e-commerce APIs
- ‚úÖ Normalize data from different sources with different schemas
- ‚úÖ Process large volumes of data in real-time
- ‚úÖ Handle data consistency and deduplication
- ‚úÖ Provide unified analytics view across all sources
- ‚úÖ Enable real-time dashboards with Apache Superset

---

## üèóÔ∏è Core Architecture

This is an **event-driven, microservices-based streaming architecture** using the Lambda architecture pattern (real-time + batch processing).

<img width="491" height="220" alt="Image" src="https://github.com/user-attachments/assets/bcd04f14-13a5-4e67-a484-fb0098e8e6ae" />

---

## üì¶ Key Components & Modules

### 1. **Ingestion Layer: `multi_api_producer.py`**

**Purpose**: Acts as the data producer, fetching data from multiple external APIs and normalizing them into Kafka topics.

**Key Functions**:

| Function | Purpose |
|----------|---------|
| `create_kafka_producer()` | Establishes Kafka connection with retry logic (10 attempts, 10s intervals) |
| `fetch_data()` | Fetches data from APIs (FakeStore, DummyJSON) |
| `normalize_product()` | Converts product schemas from different sources to unified format |
| `normalize_cart()` | Converts cart schemas to unified format |
| `normalize_user()` | Converts user schemas to unified format |
| `send_to_kafka()` | Publishes normalized data to appropriate Kafka topic |
| `main()` | Orchestrates the full pipeline (fetches all sources, normalizes, produces) |

**Data Sources**:
- **FakeStore API**: `https://fakestoreapi.com` - Simple e-commerce dataset
- **DummyJSON API**: `https://dummyjson.com` - More complex dataset with different schema

**Normalization Process**:
- Extracts common fields from both APIs
- Maps API-specific nested structures to unified format
- Adds `source` field to track data origin
- Handles different field names (e.g., `firstName` ‚Üí `firstname`)

**Output Topics**:
- `fakestore-products` - Normalized product data
- `fakestore-carts` - Normalized shopping cart data
- `fakestore-users` - Normalized user data

---

### 2. **Stream Processing Layer: Spark Processors**

#### **spark_products_processor.py**

**Purpose**: Consumes product data from Kafka, transforms it with business logic, and persists to PostgreSQL.

**Key Operations**:
```
Kafka Topic (fakestore-products)
    ‚Üì
Parse JSON with defined schema
    ‚Üì
Extract nested rating.rate and rating.count
    ‚Üì
Categorize price: <50‚Ç¨ (√©conomique), <100‚Ç¨ (moyen), ‚â•100‚Ç¨ (premium)
    ‚Üì
Deduplication by product ID
    ‚Üì
Batch write to PostgreSQL (ON CONFLICT ‚Üí UPDATE)
```

**Transformation Details**:
- Extracts nested rating structure from JSON
- Creates `price_category` column for segmentation
- Preserves `source` field for data lineage
- Batch processing with deduplication (dropDuplicates on ID)
- Handles upserts: INSERT with ON CONFLICT UPDATE clause

---

#### **spark_carts_processor.py**

**Purpose**: Consumes cart data from Kafka, explodes products, enriches with temporal data, and persists.

**Key Operations**:
```
Kafka Topic (fakestore-carts)
    ‚Üì
Parse JSON with nested product arrays
    ‚Üì
Explode products array (one row per product in cart)
    ‚Üì
Extract temporal features:
  - day_of_week (Monday, Tuesday, etc.)
  - month (January, February, etc.)
    ‚Üì
Deduplication by (cart_id, product_id)
    ‚Üì
Batch write to PostgreSQL
```

**Transformation Details**:
- Explodes products array to create detail rows
- Converts ISO date strings to Timestamp type
- Derives day-of-week and month for temporal analysis
- Composite key deduplication (cart_id + product_id)
- Handles upserts with composite key conflict detection

---

#### **process_users.py**

**Purpose**: Lightweight Kafka consumer that processes user data directly into PostgreSQL (no Spark).

**Flow**:
```
Kafka Consumer (fakestore-users topic)
    ‚Üì
For each user message:
    ‚îú‚îÄ Parse nested address and name objects
    ‚îú‚îÄ Extract all fields
    ‚îú‚îÄ Prepare SQL INSERT with ON CONFLICT
    ‚Üì
Direct insert to PostgreSQL (no batching)
```

**Why Different Approach?**:
- User data is simpler (fewer nested structures)
- Direct SQL upserts are sufficient
- Avoids Spark overhead for straightforward transformations

---

### 3. **Data Layer: PostgreSQL**

**Tables**:

```sql
products
‚îú‚îÄ id (PK)
‚îú‚îÄ title
‚îú‚îÄ price
‚îú‚îÄ category
‚îú‚îÄ rating_value (extracted from nested rating.rate)
‚îú‚îÄ rating_count (extracted from nested rating.count)
‚îú‚îÄ price_category (calculated)
‚îî‚îÄ source (fakestore or dummyjson)

cart_items
‚îú‚îÄ cart_id (PK)
‚îú‚îÄ product_id (PK)
‚îú‚îÄ userId
‚îú‚îÄ date
‚îú‚îÄ quantity
‚îú‚îÄ day_of_week (calculated)
‚îú‚îÄ month (calculated)
‚îî‚îÄ source (fakestore or dummyjson)

users
‚îú‚îÄ id (PK)
‚îú‚îÄ email (UNIQUE)
‚îú‚îÄ username
‚îú‚îÄ first_name
‚îú‚îÄ last_name
‚îú‚îÄ phone
‚îú‚îÄ address_street
‚îú‚îÄ address_city
‚îú‚îÄ address_zipcode
‚îî‚îÄ source (fakestore or dummyjson)
```

**Analytical Views** (for Superset dashboards):

| View | Purpose |
|------|---------|
| `vw_product_ratings` | Group products by category/price_category, calculate avg ratings |
| `vw_cart_analysis` | Analyze carts by day of week/month, count carts, sum quantities |
| `vw_cart_products` | Join carts with products, calculate total price per item |
| `vw_data_source_comparison` | Compare record counts and metrics across data sources |

---

### 4. **Infrastructure: Docker Compose**

**Services**:

| Service | Role | Purpose |
|---------|------|---------|
| **Zookeeper** | Coordination | Manages Kafka broker state and partitions |
| **Kafka** | Message Broker | Central event streaming platform |
| **Spark Master** | Orchestration | Coordinates Spark job execution |
| **Spark Worker** | Computation | Executes Spark tasks in parallel |
| **PostgreSQL** | Data Warehouse | Persists normalized, processed data |
| **Superset** | Visualization | Creates real-time analytics dashboards |

**Volumes Mounted**:
- `./scripts/` ‚Üí `/scripts` - Python processing scripts
- `./data/` ‚Üí `/data` - Shared data storage
- `./logs/` ‚Üí `/logs` - Application logs

---

## üîÑ Data Flow & Communication

### **End-to-End Data Flow Diagram**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FakeStore  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    API      ‚îÇ                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                              ‚îÇ
                                                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                              ‚ñº
‚îÇ DummyJSON   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  multi_api_producer.py
‚îÇ    API      ‚îÇ                                      ‚îÇ   (Python)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                      ‚îÇ
                                                      ‚îÇ
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                     ‚îÇ  Normalization & Mapping    ‚îÇ‚îÄ‚îò
                     ‚îÇ  ‚Ä¢ Extract common fields    ‚îÇ
                     ‚îÇ  ‚Ä¢ Handle API differences   ‚îÇ
                     ‚îÇ  ‚Ä¢ Add source tracking      ‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Kafka Topics:        ‚îÇ
                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                    ‚îÇ products: 100 items   ‚îÇ
                    ‚îÇ carts: 50 items       ‚îÇ
                    ‚îÇ users: 30 items       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                      ‚îÇ                      ‚îÇ
        ‚ñº                      ‚ñº                      ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇSpark    ‚îÇ         ‚îÇSpark         ‚îÇ      ‚îÇKafka        ‚îÇ
   ‚îÇProducts ‚îÇ         ‚îÇCarts         ‚îÇ      ‚îÇConsumer     ‚îÇ
   ‚îÇProcessor‚îÇ         ‚îÇProcessor     ‚îÇ      ‚îÇfor Users    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                     ‚îÇ                     ‚îÇ
        ‚îÇ JSON Parsing        ‚îÇ JSON Parsing        ‚îÇ Direct JSON
        ‚îÇ Price Categorize    ‚îÇ Array Explode       ‚îÇ Mapping
        ‚îÇ Deduplication       ‚îÇ Temporal Extract    ‚îÇ
        ‚îÇ                     ‚îÇ Deduplication       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                     ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  PostgreSQL      ‚îÇ
                    ‚îÇ  UPSERT Queries  ‚îÇ
                    ‚îÇ  ON CONFLICT     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Data Warehouse  ‚îÇ
                    ‚îÇ ‚Ä¢ products      ‚îÇ
                    ‚îÇ ‚Ä¢ carts         ‚îÇ
                    ‚îÇ ‚Ä¢ users         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Analytical Views    ‚îÇ
                    ‚îÇ ‚Ä¢ ratings analysis  ‚îÇ
                    ‚îÇ ‚Ä¢ cart patterns     ‚îÇ
                    ‚îÇ ‚Ä¢ source comparison ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Apache Superset     ‚îÇ
                    ‚îÇ Real-time Dashboard ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Component Interactions**

```
Sequential Execution Order:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1. Docker Compose Startup
   ‚îú‚îÄ Start Zookeeper
   ‚îú‚îÄ Start Kafka (depends on Zookeeper)
   ‚îú‚îÄ Start Spark Master
   ‚îú‚îÄ Start Spark Worker (depends on Master)
   ‚îú‚îÄ Start PostgreSQL
   ‚îî‚îÄ Start Superset (depends on PostgreSQL)

2. Data Ingestion Phase
   ‚îî‚îÄ Run multi_api_producer.py
      ‚îú‚îÄ Connect to Kafka (retry logic)
      ‚îú‚îÄ For each API source (FakeStore, DummyJSON)
      ‚îÇ  ‚îú‚îÄ Fetch products, carts, users
      ‚îÇ  ‚îú‚îÄ Normalize each data type
      ‚îÇ  ‚îî‚îÄ Send to Kafka topics
      ‚îî‚îÄ Publish ~280 total records across 3 topics

3. Stream Processing Phase
   ‚îú‚îÄ Start spark_products_processor.py
   ‚îÇ  ‚îú‚îÄ Listen to fakestore-products topic
   ‚îÇ  ‚îú‚îÄ For each batch:
   ‚îÇ  ‚îÇ  ‚îú‚îÄ Parse JSON
   ‚îÇ  ‚îÇ  ‚îú‚îÄ Categorize prices
   ‚îÇ  ‚îÇ  ‚îú‚îÄ Deduplicate by ID
   ‚îÇ  ‚îÇ  ‚îî‚îÄ Upsert to products table
   ‚îÇ  ‚îî‚îÄ Insert ~200 normalized products
   ‚îÇ
   ‚îú‚îÄ Start spark_carts_processor.py
   ‚îÇ  ‚îú‚îÄ Listen to fakestore-carts topic
   ‚îÇ  ‚îú‚îÄ For each batch:
   ‚îÇ  ‚îÇ  ‚îú‚îÄ Parse JSON
   ‚îÇ  ‚îÇ  ‚îú‚îÄ Explode products array
   ‚îÇ  ‚îÇ  ‚îú‚îÄ Extract temporal data
   ‚îÇ  ‚îÇ  ‚îú‚îÄ Deduplicate by (cart_id, product_id)
   ‚îÇ  ‚îÇ  ‚îî‚îÄ Upsert to cart_items table
   ‚îÇ  ‚îî‚îÄ Insert ~500+ cart item rows
   ‚îÇ
   ‚îî‚îÄ Start process_users.py
      ‚îú‚îÄ Listen to fakestore-users topic
      ‚îî‚îÄ For each message:
         ‚îú‚îÄ Parse nested structure
         ‚îî‚îÄ Upsert to users table

4. Analytics Phase
   ‚îú‚îÄ PostgreSQL creates views from normalized data
   ‚îî‚îÄ Superset queries views for real-time dashboards
```

---

## üõ†Ô∏è Tech Stack & Dependencies

### **Backend Technologies**

| Component | Version | Purpose |
|-----------|---------|---------|
| **Python** | 3.9 | Data processing scripting |
| **Apache Kafka** | 2.8 | Distributed event streaming |
| **Apache Spark** | 3.3.1 | Large-scale stream processing |
| **PostgreSQL** | 13 | Relational data warehouse |
| **Apache Superset** | Latest | Analytics and visualization |
| **Zookeeper** | 3.8 | Distributed coordination |

### **Python Dependencies**

```
requests==2.28.2          # HTTP API calls
kafka-python==2.0.2       # Kafka client library
pyspark==3.1.2            # Spark Python API
pandas==1.5.3             # Data manipulation
psycopg2-binary==2.9.6    # PostgreSQL adapter
```

### **Why These Technologies?**

1. **Kafka**: 
   - ‚úÖ Decouples producers from consumers
   - ‚úÖ Provides durable, replayable event log
   - ‚úÖ Supports multiple consumers independently
   - ‚úÖ Natural fit for real-time streaming

2. **Spark**: 
   - ‚úÖ Distributed processing for large datasets
   - ‚úÖ Structured Streaming API for real-time data
   - ‚úÖ SQL support for complex transformations
   - ‚úÖ Integration with PostgreSQL

3. **PostgreSQL**: 
   - ‚úÖ ACID transactions guarantee data consistency
   - ‚úÖ ON CONFLICT clause for efficient upserts
   - ‚úÖ SQL views for analytical queries
   - ‚úÖ Integrates well with Superset

4. **Superset**: 
   - ‚úÖ Real-time dashboard capabilities
   - ‚úÖ Multi-source data visualization
   - ‚úÖ Interactive exploration tools
   - ‚úÖ No-code dashboard creation

---

## üöÄ Execution Flow - Typical Workflow

### **Complete Request-to-Dashboard Flow**

```
START: User wants to analyze e-commerce trends
‚îÇ
‚îú‚îÄ 1. START INFRASTRUCTURE
‚îÇ  ‚îú‚îÄ docker-compose up
‚îÇ  ‚îú‚îÄ Zookeeper starts on 2181
‚îÇ  ‚îú‚îÄ Kafka starts on 9092
‚îÇ  ‚îú‚îÄ Spark Master on 8083, Worker on 8081
‚îÇ  ‚îú‚îÄ PostgreSQL on 5432
‚îÇ  ‚îî‚îÄ Superset on 8088
‚îÇ
‚îú‚îÄ 2. DATA INGESTION (multi_api_producer.py)
‚îÇ  ‚îú‚îÄ Connect to Kafka with retry logic
‚îÇ  ‚îú‚îÄ For FakeStore API:
‚îÇ  ‚îÇ  ‚îú‚îÄ GET https://fakestoreapi.com/products
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Returns: {id, title, price, category, rating}
‚îÇ  ‚îÇ  ‚îú‚îÄ Normalize: {id, title, price, category, rating: {rate, count}, source: 'fakestore'}
‚îÇ  ‚îÇ  ‚îú‚îÄ Send to fakestore-products topic
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ GET https://fakestoreapi.com/carts
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Returns: {id, userId, date, products: [{productId, quantity}]}
‚îÇ  ‚îÇ  ‚îú‚îÄ Normalize: Same structure + source field
‚îÇ  ‚îÇ  ‚îú‚îÄ Send to fakestore-carts topic
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ GET https://fakestoreapi.com/users
‚îÇ  ‚îÇ     ‚îî‚îÄ Returns: {id, email, username, name: {firstname, lastname}, ...}
‚îÇ  ‚îÇ     ‚îî‚îÄ Normalize: Extract nested fields, add source
‚îÇ  ‚îÇ     ‚îî‚îÄ Send to fakestore-users topic
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ For DummyJSON API:
‚îÇ  ‚îÇ  ‚îú‚îÄ GET https://dummyjson.com/products?limit=100
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Returns: {products: [{id, title, price, category, rating, ...}]}
‚îÇ  ‚îÇ  ‚îú‚îÄ Normalize: Map to FakeStore schema, add source: 'dummyjson'
‚îÇ  ‚îÇ  ‚îú‚îÄ Send to fakestore-products topic
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ GET https://dummyjson.com/carts?limit=100
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Returns: {carts: [{id, userId, date, products}]}
‚îÇ  ‚îÇ  ‚îú‚îÄ Normalize: Map nested structures, add source
‚îÇ  ‚îÇ  ‚îú‚îÄ Send to fakestore-carts topic
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Similar for users...
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ Kafka now has ~280 total messages across 3 topics
‚îÇ
‚îú‚îÄ 3. STREAM PROCESSING
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ spark_products_processor.py:
‚îÇ  ‚îÇ  ‚îú‚îÄ Subscribe to fakestore-products topic
‚îÇ  ‚îÇ  ‚îú‚îÄ ReadStream from Kafka with earliest offsets
‚îÇ  ‚îÇ  ‚îú‚îÄ Parse JSON using product_schema
‚îÇ  ‚îÇ  ‚îú‚îÄ For each batch:
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Extract rating.rate ‚Üí rating_value
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Extract rating.count ‚Üí rating_count
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Calculate price_category:
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ price < 50 ‚Üí '√©conomique'
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ price < 100 ‚Üí 'moyen'
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ price ‚â• 100 ‚Üí 'premium'
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Deduplicate on product ID (keep latest)
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Write batch to PostgreSQL:
‚îÇ  ‚îÇ  ‚îÇ     ‚îî‚îÄ INSERT INTO products (...) VALUES (...)
‚îÇ  ‚îÇ  ‚îÇ        ON CONFLICT (id) DO UPDATE SET ...
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Result: ~200 products in products table
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ spark_carts_processor.py:
‚îÇ  ‚îÇ  ‚îú‚îÄ Subscribe to fakestore-carts topic
‚îÇ  ‚îÇ  ‚îú‚îÄ ReadStream from Kafka
‚îÇ  ‚îÇ  ‚îú‚îÄ For each batch:
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Parse JSON using cart_schema
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Explode products array:
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Original: {cart_id: 1, products: [{id:10, qty:2}, {id:20, qty:1}]}
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ After explosion:
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ     ‚îú‚îÄ Row 1: cart_id:1, product_id:10, qty:2
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ     ‚îî‚îÄ Row 2: cart_id:1, product_id:20, qty:1
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Extract temporal data:
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ 2024-01-15 ‚Üí day_of_week: 'Monday', month: 'January'
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Deduplicate on (cart_id, product_id)
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Write to PostgreSQL:
‚îÇ  ‚îÇ  ‚îÇ     ‚îî‚îÄ INSERT INTO cart_items (...) VALUES (...)
‚îÇ  ‚îÇ  ‚îÇ        ON CONFLICT (cart_id, product_id) DO UPDATE SET ...
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Result: ~500+ cart items in cart_items table
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ process_users.py:
‚îÇ     ‚îú‚îÄ Subscribe to fakestore-users topic
‚îÇ     ‚îú‚îÄ KafkaConsumer: group_id='user-processor-group'
‚îÇ     ‚îú‚îÄ For each message:
‚îÇ     ‚îÇ  ‚îú‚îÄ Parse nested user object
‚îÇ     ‚îÇ  ‚îú‚îÄ Extract: name.firstname ‚Üí first_name, address.street ‚Üí address_street
‚îÇ     ‚îÇ  ‚îî‚îÄ INSERT or UPDATE in users table
‚îÇ     ‚îÇ
‚îÇ     ‚îî‚îÄ Result: ~60 users in users table
‚îÇ
‚îú‚îÄ 4. ANALYTICAL PROCESSING
‚îÇ  ‚îú‚îÄ PostgreSQL creates views from normalized data:
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ vw_product_ratings:
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ SELECT category, price_category, source, 
‚îÇ  ‚îÇ  ‚îÇ        COUNT(*) as product_count, AVG(rating_value) as avg_rating
‚îÇ  ‚îÇ  ‚îÇ     FROM products
‚îÇ  ‚îÇ  ‚îÇ     GROUP BY category, price_category, source
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ vw_cart_analysis:
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ SELECT day_of_week, month, source,
‚îÇ  ‚îÇ  ‚îÇ        COUNT(DISTINCT cart_id) as cart_count, SUM(quantity) as total_items
‚îÇ  ‚îÇ  ‚îÇ     FROM cart_items
‚îÇ  ‚îÇ  ‚îÇ     GROUP BY day_of_week, month, source
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ vw_cart_products:
‚îÇ  ‚îÇ     ‚îî‚îÄ SELECT cart_id, userId, p.title, c.quantity, 
‚îÇ  ‚îÇ           (p.price * c.quantity) as total_price
‚îÇ  ‚îÇ        FROM cart_items c JOIN products p ON ...
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ Views now contain aggregated analytics
‚îÇ
‚îú‚îÄ 5. VISUALIZATION (Apache Superset)
‚îÇ  ‚îú‚îÄ User opens Superset at http://localhost:8088
‚îÇ  ‚îú‚îÄ Connect Superset to PostgreSQL
‚îÇ  ‚îú‚îÄ Create dashboards from views:
‚îÇ  ‚îÇ  ‚îú‚îÄ Dashboard 1: Product Analysis
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Chart: Average Rating by Category (vw_product_ratings)
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Chart: Price Distribution (vw_product_ratings)
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Chart: Source Comparison (vw_data_source_comparison)
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Dashboard 2: Shopping Patterns
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Chart: Carts by Day of Week (vw_cart_analysis)
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ Chart: Items Sold by Month (vw_cart_analysis)
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ Chart: Peak Shopping Times
‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Dashboard 3: Multi-Source Comparison
‚îÇ  ‚îÇ     ‚îú‚îÄ Table: Record Counts by Source
‚îÇ  ‚îÇ     ‚îú‚îÄ Chart: Average Price Differences
‚îÇ  ‚îÇ     ‚îî‚îÄ Chart: Source Coverage
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ Real-time dashboards update as new data arrives
‚îÇ
‚îî‚îÄ END: User has real-time insights into merged e-commerce data
```

---

## üóÇÔ∏è Project Structure

```
data-pipeline/
‚îú‚îÄ‚îÄ docker-compose.yml               # Orchestrates all services
‚îú‚îÄ‚îÄ Dockerfile                       # Python 3.9 + Java environment
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ README.md                        # This file
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ init-db.sql                 # PostgreSQL schema & views
‚îÇ   ‚îî‚îÄ‚îÄ superset_config.py           # Superset configuration
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ python/
‚îÇ       ‚îú‚îÄ‚îÄ multi_api_producer.py    # Data ingestion (Kafka producer)
‚îÇ       ‚îú‚îÄ‚îÄ process_users.py         # User data processor (Kafka consumer)
‚îÇ       ‚îú‚îÄ‚îÄ spark_products_processor.py    # Product data processor (Spark)
‚îÇ       ‚îî‚îÄ‚îÄ spark_carts_processor.py       # Cart data processor (Spark)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ postgres/                   # PostgreSQL data volume
‚îÇ   ‚îî‚îÄ‚îÄ (other data files)
‚îÇ
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ (application logs)
```

---

## üöÄ Quick Start

### **Prerequisites**
- Docker and Docker Compose
- 8GB RAM minimum
- ~2GB disk space

### **Installation & Startup**

```bash
# 1. Navigate to project directory
cd data-pipeline

# 2. Build and start all services
docker-compose up -d

# 3. Wait for all services to be healthy (~30 seconds)
docker-compose ps

# 4. Run data ingestion
docker-compose exec -T spark-master python /scripts/python/multi_api_producer.py

# 5. Run stream processors (in separate terminals)
docker-compose exec -T spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \
  --master spark://spark-master:7077 \
  /scripts/python/spark_products_processor.py

docker-compose exec -T spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \
  --master spark://spark-master:7077 \
  /scripts/python/spark_carts_processor.py

docker exec -T spark-master python /scripts/python/process_users.py

# 6. Access Superset
# Open browser: http://localhost:8088
# Login: admin/admin
```

---

## üìä Database & Queries

### **Querying Results**

```bash
# Connect to PostgreSQL
docker exec -it postgres psql -U postgres_user -d data_fakestore_db

# View normalized products
SELECT * FROM products LIMIT 10;

# Analyze product ratings
SELECT * FROM vw_product_ratings;

# Analyze shopping patterns
SELECT * FROM vw_cart_analysis;

# Compare data sources
SELECT * FROM vw_data_source_comparison;
```

### **Key Metrics Available**

1. **Product Analytics**
   - Average rating by category
   - Price distribution by source
   - Product count by price category

2. **Shopping Behavior**
   - Peak shopping days/months
   - Items purchased per cart
   - Cart value distribution

3. **Data Quality**
   - Record counts by source
   - Coverage comparison
   - Data freshness

---

## üîê Security & Best Practices

1. **Credentials**: Database credentials hardcoded in config (for demo only)
   - ‚ö†Ô∏è Change in production
   - Use environment variables or secrets manager

2. **Network**: All services on internal Docker network
   - ‚úÖ External exposure only on necessary ports

3. **Data Validation**: 
   - ‚úÖ ON CONFLICT handling prevents duplicates
   - ‚úÖ Schema validation in Spark processors

4. **Error Handling**:
   - ‚úÖ Retry logic in Kafka connections (10 attempts)
   - ‚úÖ Exception handling in all processors
   - ‚úÖ Rollback on database errors

---

## üõ†Ô∏è Troubleshooting

### **Common Issues**

| Issue | Cause | Solution |
|-------|-------|----------|
| Kafka connection failed | Kafka not fully started | Wait 30s, check `docker-compose ps` |
| PostgreSQL not accessible | Database not initialized | Check `docker logs postgres` |
| Spark job fails | Missing dependencies | Verify `--packages` in spark-submit |
| No data in tables | Producer not run | Execute `multi_api_producer.py` first |
| Duplicate records | Missing deduplication | Check processor's `dropDuplicates()` call |

### **Debugging**

```bash
# Check service health
docker-compose ps

# View logs
docker logs kafka
docker logs postgres
docker logs superset

# Test Kafka connectivity
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server kafka:9092 \
  --topic fakestore-products \
  --from-beginning

# Test PostgreSQL
docker exec -it postgres psql -U postgres_user -d data_fakestore_db \
  -c "SELECT COUNT(*) FROM products;"
```

---

## üìà Monitoring & Metrics

### **Key Performance Indicators**

- **Ingestion Rate**: Records/second from APIs
- **Processing Latency**: Time from Kafka to PostgreSQL
- **Data Freshness**: Time since last update
- **Error Rate**: Failed records percentage
- **Throughput**: Total records processed

### **Dashboards Available**

1. **Spark Master UI**: `http://localhost:8083`
   - Job status and progress
   - Executor metrics

2. **Superset**: `http://localhost:8088`
   - Real-time analytics
   - Custom visualizations

---

## üéØ Architecture Patterns Used

1. **Event-Driven Architecture**: Kafka decouples producers/consumers
2. **Lambda Architecture**: Real-time stream + batch processing
3. **Data Normalization**: Common schema from heterogeneous sources
4. **Microservices**: Independent processors for each data type
5. **CQRS**: Separate read (views) from write (tables)

---

## üìö Learning Outcomes

By studying this project, you'll learn:

‚úÖ **Event Streaming**: Apache Kafka fundamentals  
‚úÖ **Stream Processing**: Apache Spark Structured Streaming  
‚úÖ **Data Engineering**: ETL/ELT patterns  
‚úÖ **Data Normalization**: Handling heterogeneous schemas  
‚úÖ **Real-time Analytics**: Streaming to data warehouse  
‚úÖ **Docker Orchestration**: Multi-container deployments  
‚úÖ **SQL & Analytics**: Data warehousing concepts  
‚úÖ **Python Data Processing**: PySpark and psycopg2  

---


