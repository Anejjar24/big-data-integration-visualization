# Data Pipeline - Multi-Source Real-Time Data Integration

## ğŸ“‹ Quick Overview

**Data Pipeline** is a sophisticated **event-driven streaming data integration system** that fetches data from multiple e-commerce APIs (FakeStore and DummyJSON), processes it in real-time using Apache Kafka and Apache Spark, and persists normalized data into PostgreSQL for analytics and visualization.

**Problem Solved**: 
- âœ… Integrate data from multiple heterogeneous e-commerce APIs
- âœ… Normalize data from different sources with different schemas
- âœ… Process large volumes of data in real-time
- âœ… Handle data consistency and deduplication
- âœ… Provide unified analytics view across all sources
- âœ… Enable real-time dashboards with Apache Superset

---

## ğŸ—ï¸ Core Architecture

This is an **event-driven, microservices-based streaming architecture** using the Lambda architecture pattern (real-time + batch processing).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATA SOURCES LAYER                                 â”‚
â”‚                  (Multiple External APIs)                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  FakeStore API  â”‚              â”‚  DummyJSON API  â”‚                      â”‚
â”‚  â”‚  - Products     â”‚              â”‚  - Products     â”‚                      â”‚
â”‚  â”‚  - Carts        â”‚              â”‚  - Carts        â”‚                      â”‚
â”‚  â”‚  - Users        â”‚              â”‚  - Users        â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                â”‚ HTTP API Calls
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INGESTION LAYER (Kafka Producer)                       â”‚
â”‚                    multi_api_producer.py                                    â”‚
â”‚  - Fetches data from multiple APIs                                         â”‚
â”‚  - Normalizes heterogeneous schemas                                        â”‚
â”‚  - Produces to Kafka topics                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Three Kafka Topics:                                     â”‚               â”‚
â”‚  â”‚ â€¢ fakestore-products (normalized product data)          â”‚               â”‚
â”‚  â”‚ â€¢ fakestore-carts (normalized cart data)                â”‚               â”‚
â”‚  â”‚ â€¢ fakestore-users (normalized user data)                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Kafka Streaming
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAM PROCESSING LAYER (Apache Spark)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚spark_products_       â”‚  â”‚spark_carts_          â”‚  process_users.py     â”‚
â”‚  â”‚processor.py          â”‚  â”‚processor.py          â”‚  (Kafka Consumer)     â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚                        â”‚
â”‚  â”‚â€¢ Parse JSON schema   â”‚  â”‚â€¢ Parse JSON schema   â”‚  â€¢ Consume from       â”‚
â”‚  â”‚â€¢ Extract nested      â”‚  â”‚â€¢ Explode products    â”‚    Kafka              â”‚
â”‚  â”‚  ratings             â”‚  â”‚â€¢ Calculate day/month â”‚  â€¢ Direct insert to   â”‚
â”‚  â”‚â€¢ Create price        â”‚  â”‚â€¢ Create price        â”‚    PostgreSQL         â”‚
â”‚  â”‚  categories          â”‚  â”‚  categories          â”‚                        â”‚
â”‚  â”‚â€¢ Deduplication       â”‚  â”‚â€¢ Deduplication       â”‚                        â”‚
â”‚  â”‚â€¢ Batch write         â”‚  â”‚â€¢ Batch write         â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚             â”‚                         â”‚                        â”‚           â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                              â”‚ SQL UPSERT Queries                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA WAREHOUSE LAYER (PostgreSQL)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   products      â”‚  â”‚   cart_items     â”‚  â”‚      users       â”‚          â”‚
â”‚  â”‚  (normalized)   â”‚  â”‚  (normalized)    â”‚  â”‚  (normalized)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                              â”‚
â”‚  Analytical Views:                                                          â”‚
â”‚  â€¢ vw_product_ratings - Product analysis by category                      â”‚
â”‚  â€¢ vw_cart_analysis - Shopping patterns by time                           â”‚
â”‚  â€¢ vw_cart_products - Cart-product joins                                  â”‚
â”‚  â€¢ vw_data_source_comparison - Cross-source statistics                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ JDBC Connection
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VISUALIZATION LAYER (Apache Superset)                    â”‚
â”‚                     Real-time Analytics Dashboard                          â”‚
â”‚  - Interactive charts and visualizations                                   â”‚
â”‚  - Real-time metric updates                                                â”‚
â”‚  - Multi-source comparison dashboards                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Key Components & Modules

### 1. **Ingestion Layer: `multi_api_producer.py`**

**Purpose**: Acts as the data producer, fetching data from multiple external APIs and normalizing them into Kafka topics.

**Key Functions**:

| Function | Purpose |
|----------|---------|
| `create_kafka_producer()` | Establishes Kafka connection with retry logic (10 attempts, 10s intervals) |
| `fetch_data()` | Fetches data from APIs (FakeStore, DummyJSON) |
| `normalize_product()` | Converts product schemas from different sources to unified format |
| `normalize_cart()` | Converts cart schemas to unified format |
| `normalize_user()` | Converts user schemas to unified format |
| `send_to_kafka()` | Publishes normalized data to appropriate Kafka topic |
| `main()` | Orchestrates the full pipeline (fetches all sources, normalizes, produces) |

**Data Sources**:
- **FakeStore API**: `https://fakestoreapi.com` - Simple e-commerce dataset
- **DummyJSON API**: `https://dummyjson.com` - More complex dataset with different schema

**Normalization Process**:
- Extracts common fields from both APIs
- Maps API-specific nested structures to unified format
- Adds `source` field to track data origin
- Handles different field names (e.g., `firstName` â†’ `firstname`)

**Output Topics**:
- `fakestore-products` - Normalized product data
- `fakestore-carts` - Normalized shopping cart data
- `fakestore-users` - Normalized user data

---

### 2. **Stream Processing Layer: Spark Processors**

#### **spark_products_processor.py**

**Purpose**: Consumes product data from Kafka, transforms it with business logic, and persists to PostgreSQL.

**Key Operations**:
```
Kafka Topic (fakestore-products)
    â†“
Parse JSON with defined schema
    â†“
Extract nested rating.rate and rating.count
    â†“
Categorize price: <50â‚¬ (Ã©conomique), <100â‚¬ (moyen), â‰¥100â‚¬ (premium)
    â†“
Deduplication by product ID
    â†“
Batch write to PostgreSQL (ON CONFLICT â†’ UPDATE)
```

**Transformation Details**:
- Extracts nested rating structure from JSON
- Creates `price_category` column for segmentation
- Preserves `source` field for data lineage
- Batch processing with deduplication (dropDuplicates on ID)
- Handles upserts: INSERT with ON CONFLICT UPDATE clause

---

#### **spark_carts_processor.py**

**Purpose**: Consumes cart data from Kafka, explodes products, enriches with temporal data, and persists.

**Key Operations**:
```
Kafka Topic (fakestore-carts)
    â†“
Parse JSON with nested product arrays
    â†“
Explode products array (one row per product in cart)
    â†“
Extract temporal features:
  - day_of_week (Monday, Tuesday, etc.)
  - month (January, February, etc.)
    â†“
Deduplication by (cart_id, product_id)
    â†“
Batch write to PostgreSQL
```

**Transformation Details**:
- Explodes products array to create detail rows
- Converts ISO date strings to Timestamp type
- Derives day-of-week and month for temporal analysis
- Composite key deduplication (cart_id + product_id)
- Handles upserts with composite key conflict detection

---

#### **process_users.py**

**Purpose**: Lightweight Kafka consumer that processes user data directly into PostgreSQL (no Spark).

**Flow**:
```
Kafka Consumer (fakestore-users topic)
    â†“
For each user message:
    â”œâ”€ Parse nested address and name objects
    â”œâ”€ Extract all fields
    â”œâ”€ Prepare SQL INSERT with ON CONFLICT
    â†“
Direct insert to PostgreSQL (no batching)
```

**Why Different Approach?**:
- User data is simpler (fewer nested structures)
- Direct SQL upserts are sufficient
- Avoids Spark overhead for straightforward transformations

---

### 3. **Data Layer: PostgreSQL**

**Tables**:

```sql
products
â”œâ”€ id (PK)
â”œâ”€ title
â”œâ”€ price
â”œâ”€ category
â”œâ”€ rating_value (extracted from nested rating.rate)
â”œâ”€ rating_count (extracted from nested rating.count)
â”œâ”€ price_category (calculated)
â””â”€ source (fakestore or dummyjson)

cart_items
â”œâ”€ cart_id (PK)
â”œâ”€ product_id (PK)
â”œâ”€ userId
â”œâ”€ date
â”œâ”€ quantity
â”œâ”€ day_of_week (calculated)
â”œâ”€ month (calculated)
â””â”€ source (fakestore or dummyjson)

users
â”œâ”€ id (PK)
â”œâ”€ email (UNIQUE)
â”œâ”€ username
â”œâ”€ first_name
â”œâ”€ last_name
â”œâ”€ phone
â”œâ”€ address_street
â”œâ”€ address_city
â”œâ”€ address_zipcode
â””â”€ source (fakestore or dummyjson)
```

**Analytical Views** (for Superset dashboards):

| View | Purpose |
|------|---------|
| `vw_product_ratings` | Group products by category/price_category, calculate avg ratings |
| `vw_cart_analysis` | Analyze carts by day of week/month, count carts, sum quantities |
| `vw_cart_products` | Join carts with products, calculate total price per item |
| `vw_data_source_comparison` | Compare record counts and metrics across data sources |

---

### 4. **Infrastructure: Docker Compose**

**Services**:

| Service | Role | Purpose |
|---------|------|---------|
| **Zookeeper** | Coordination | Manages Kafka broker state and partitions |
| **Kafka** | Message Broker | Central event streaming platform |
| **Spark Master** | Orchestration | Coordinates Spark job execution |
| **Spark Worker** | Computation | Executes Spark tasks in parallel |
| **PostgreSQL** | Data Warehouse | Persists normalized, processed data |
| **Superset** | Visualization | Creates real-time analytics dashboards |

**Volumes Mounted**:
- `./scripts/` â†’ `/scripts` - Python processing scripts
- `./data/` â†’ `/data` - Shared data storage
- `./logs/` â†’ `/logs` - Application logs

---

## ğŸ”„ Data Flow & Communication

### **End-to-End Data Flow Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FakeStore  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    API      â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
                                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â–¼
â”‚ DummyJSON   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  multi_api_producer.py
â”‚    API      â”‚                                      â”‚   (Python)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
                                                      â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                     â”‚  Normalization & Mapping    â”‚â”€â”˜
                     â”‚  â€¢ Extract common fields    â”‚
                     â”‚  â€¢ Handle API differences   â”‚
                     â”‚  â€¢ Add source tracking      â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Kafka Topics:        â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ products: 100 items   â”‚
                    â”‚ carts: 50 items       â”‚
                    â”‚ users: 30 items       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Spark    â”‚         â”‚Spark         â”‚      â”‚Kafka        â”‚
   â”‚Products â”‚         â”‚Carts         â”‚      â”‚Consumer     â”‚
   â”‚Processorâ”‚         â”‚Processor     â”‚      â”‚for Users    â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â”‚ JSON Parsing        â”‚ JSON Parsing        â”‚ Direct JSON
        â”‚ Price Categorize    â”‚ Array Explode       â”‚ Mapping
        â”‚ Deduplication       â”‚ Temporal Extract    â”‚
        â”‚                     â”‚ Deduplication       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  PostgreSQL      â”‚
                    â”‚  UPSERT Queries  â”‚
                    â”‚  ON CONFLICT     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Data Warehouse  â”‚
                    â”‚ â€¢ products      â”‚
                    â”‚ â€¢ carts         â”‚
                    â”‚ â€¢ users         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Analytical Views    â”‚
                    â”‚ â€¢ ratings analysis  â”‚
                    â”‚ â€¢ cart patterns     â”‚
                    â”‚ â€¢ source comparison â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Apache Superset     â”‚
                    â”‚ Real-time Dashboard â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Component Interactions**

```
Sequential Execution Order:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Docker Compose Startup
   â”œâ”€ Start Zookeeper
   â”œâ”€ Start Kafka (depends on Zookeeper)
   â”œâ”€ Start Spark Master
   â”œâ”€ Start Spark Worker (depends on Master)
   â”œâ”€ Start PostgreSQL
   â””â”€ Start Superset (depends on PostgreSQL)

2. Data Ingestion Phase
   â””â”€ Run multi_api_producer.py
      â”œâ”€ Connect to Kafka (retry logic)
      â”œâ”€ For each API source (FakeStore, DummyJSON)
      â”‚  â”œâ”€ Fetch products, carts, users
      â”‚  â”œâ”€ Normalize each data type
      â”‚  â””â”€ Send to Kafka topics
      â””â”€ Publish ~280 total records across 3 topics

3. Stream Processing Phase
   â”œâ”€ Start spark_products_processor.py
   â”‚  â”œâ”€ Listen to fakestore-products topic
   â”‚  â”œâ”€ For each batch:
   â”‚  â”‚  â”œâ”€ Parse JSON
   â”‚  â”‚  â”œâ”€ Categorize prices
   â”‚  â”‚  â”œâ”€ Deduplicate by ID
   â”‚  â”‚  â””â”€ Upsert to products table
   â”‚  â””â”€ Insert ~200 normalized products
   â”‚
   â”œâ”€ Start spark_carts_processor.py
   â”‚  â”œâ”€ Listen to fakestore-carts topic
   â”‚  â”œâ”€ For each batch:
   â”‚  â”‚  â”œâ”€ Parse JSON
   â”‚  â”‚  â”œâ”€ Explode products array
   â”‚  â”‚  â”œâ”€ Extract temporal data
   â”‚  â”‚  â”œâ”€ Deduplicate by (cart_id, product_id)
   â”‚  â”‚  â””â”€ Upsert to cart_items table
   â”‚  â””â”€ Insert ~500+ cart item rows
   â”‚
   â””â”€ Start process_users.py
      â”œâ”€ Listen to fakestore-users topic
      â””â”€ For each message:
         â”œâ”€ Parse nested structure
         â””â”€ Upsert to users table

4. Analytics Phase
   â”œâ”€ PostgreSQL creates views from normalized data
   â””â”€ Superset queries views for real-time dashboards
```

---

## ğŸ› ï¸ Tech Stack & Dependencies

### **Backend Technologies**

| Component | Version | Purpose |
|-----------|---------|---------|
| **Python** | 3.9 | Data processing scripting |
| **Apache Kafka** | 2.8 | Distributed event streaming |
| **Apache Spark** | 3.3.1 | Large-scale stream processing |
| **PostgreSQL** | 13 | Relational data warehouse |
| **Apache Superset** | Latest | Analytics and visualization |
| **Zookeeper** | 3.8 | Distributed coordination |

### **Python Dependencies**

```
requests==2.28.2          # HTTP API calls
kafka-python==2.0.2       # Kafka client library
pyspark==3.1.2            # Spark Python API
pandas==1.5.3             # Data manipulation
psycopg2-binary==2.9.6    # PostgreSQL adapter
```

### **Why These Technologies?**

1. **Kafka**: 
   - âœ… Decouples producers from consumers
   - âœ… Provides durable, replayable event log
   - âœ… Supports multiple consumers independently
   - âœ… Natural fit for real-time streaming

2. **Spark**: 
   - âœ… Distributed processing for large datasets
   - âœ… Structured Streaming API for real-time data
   - âœ… SQL support for complex transformations
   - âœ… Integration with PostgreSQL

3. **PostgreSQL**: 
   - âœ… ACID transactions guarantee data consistency
   - âœ… ON CONFLICT clause for efficient upserts
   - âœ… SQL views for analytical queries
   - âœ… Integrates well with Superset

4. **Superset**: 
   - âœ… Real-time dashboard capabilities
   - âœ… Multi-source data visualization
   - âœ… Interactive exploration tools
   - âœ… No-code dashboard creation

---

## ğŸš€ Execution Flow - Typical Workflow

### **Complete Request-to-Dashboard Flow**

```
START: User wants to analyze e-commerce trends
â”‚
â”œâ”€ 1. START INFRASTRUCTURE
â”‚  â”œâ”€ docker-compose up
â”‚  â”œâ”€ Zookeeper starts on 2181
â”‚  â”œâ”€ Kafka starts on 9092
â”‚  â”œâ”€ Spark Master on 8083, Worker on 8081
â”‚  â”œâ”€ PostgreSQL on 5432
â”‚  â””â”€ Superset on 8088
â”‚
â”œâ”€ 2. DATA INGESTION (multi_api_producer.py)
â”‚  â”œâ”€ Connect to Kafka with retry logic
â”‚  â”œâ”€ For FakeStore API:
â”‚  â”‚  â”œâ”€ GET https://fakestoreapi.com/products
â”‚  â”‚  â”‚  â””â”€ Returns: {id, title, price, category, rating}
â”‚  â”‚  â”œâ”€ Normalize: {id, title, price, category, rating: {rate, count}, source: 'fakestore'}
â”‚  â”‚  â”œâ”€ Send to fakestore-products topic
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ GET https://fakestoreapi.com/carts
â”‚  â”‚  â”‚  â””â”€ Returns: {id, userId, date, products: [{productId, quantity}]}
â”‚  â”‚  â”œâ”€ Normalize: Same structure + source field
â”‚  â”‚  â”œâ”€ Send to fakestore-carts topic
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ GET https://fakestoreapi.com/users
â”‚  â”‚     â””â”€ Returns: {id, email, username, name: {firstname, lastname}, ...}
â”‚  â”‚     â””â”€ Normalize: Extract nested fields, add source
â”‚  â”‚     â””â”€ Send to fakestore-users topic
â”‚  â”‚
â”‚  â”œâ”€ For DummyJSON API:
â”‚  â”‚  â”œâ”€ GET https://dummyjson.com/products?limit=100
â”‚  â”‚  â”‚  â””â”€ Returns: {products: [{id, title, price, category, rating, ...}]}
â”‚  â”‚  â”œâ”€ Normalize: Map to FakeStore schema, add source: 'dummyjson'
â”‚  â”‚  â”œâ”€ Send to fakestore-products topic
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ GET https://dummyjson.com/carts?limit=100
â”‚  â”‚  â”‚  â””â”€ Returns: {carts: [{id, userId, date, products}]}
â”‚  â”‚  â”œâ”€ Normalize: Map nested structures, add source
â”‚  â”‚  â”œâ”€ Send to fakestore-carts topic
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Similar for users...
â”‚  â”‚
â”‚  â””â”€ Kafka now has ~280 total messages across 3 topics
â”‚
â”œâ”€ 3. STREAM PROCESSING
â”‚  â”‚
â”‚  â”œâ”€ spark_products_processor.py:
â”‚  â”‚  â”œâ”€ Subscribe to fakestore-products topic
â”‚  â”‚  â”œâ”€ ReadStream from Kafka with earliest offsets
â”‚  â”‚  â”œâ”€ Parse JSON using product_schema
â”‚  â”‚  â”œâ”€ For each batch:
â”‚  â”‚  â”‚  â”œâ”€ Extract rating.rate â†’ rating_value
â”‚  â”‚  â”‚  â”œâ”€ Extract rating.count â†’ rating_count
â”‚  â”‚  â”‚  â”œâ”€ Calculate price_category:
â”‚  â”‚  â”‚  â”‚  â”œâ”€ price < 50 â†’ 'Ã©conomique'
â”‚  â”‚  â”‚  â”‚  â”œâ”€ price < 100 â†’ 'moyen'
â”‚  â”‚  â”‚  â”‚  â””â”€ price â‰¥ 100 â†’ 'premium'
â”‚  â”‚  â”‚  â”œâ”€ Deduplicate on product ID (keep latest)
â”‚  â”‚  â”‚  â””â”€ Write batch to PostgreSQL:
â”‚  â”‚  â”‚     â””â”€ INSERT INTO products (...) VALUES (...)
â”‚  â”‚  â”‚        ON CONFLICT (id) DO UPDATE SET ...
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Result: ~200 products in products table
â”‚  â”‚
â”‚  â”œâ”€ spark_carts_processor.py:
â”‚  â”‚  â”œâ”€ Subscribe to fakestore-carts topic
â”‚  â”‚  â”œâ”€ ReadStream from Kafka
â”‚  â”‚  â”œâ”€ For each batch:
â”‚  â”‚  â”‚  â”œâ”€ Parse JSON using cart_schema
â”‚  â”‚  â”‚  â”œâ”€ Explode products array:
â”‚  â”‚  â”‚  â”‚  â”œâ”€ Original: {cart_id: 1, products: [{id:10, qty:2}, {id:20, qty:1}]}
â”‚  â”‚  â”‚  â”‚  â””â”€ After explosion:
â”‚  â”‚  â”‚  â”‚     â”œâ”€ Row 1: cart_id:1, product_id:10, qty:2
â”‚  â”‚  â”‚  â”‚     â””â”€ Row 2: cart_id:1, product_id:20, qty:1
â”‚  â”‚  â”‚  â”œâ”€ Extract temporal data:
â”‚  â”‚  â”‚  â”‚  â”œâ”€ 2024-01-15 â†’ day_of_week: 'Monday', month: 'January'
â”‚  â”‚  â”‚  â”œâ”€ Deduplicate on (cart_id, product_id)
â”‚  â”‚  â”‚  â””â”€ Write to PostgreSQL:
â”‚  â”‚  â”‚     â””â”€ INSERT INTO cart_items (...) VALUES (...)
â”‚  â”‚  â”‚        ON CONFLICT (cart_id, product_id) DO UPDATE SET ...
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Result: ~500+ cart items in cart_items table
â”‚  â”‚
â”‚  â””â”€ process_users.py:
â”‚     â”œâ”€ Subscribe to fakestore-users topic
â”‚     â”œâ”€ KafkaConsumer: group_id='user-processor-group'
â”‚     â”œâ”€ For each message:
â”‚     â”‚  â”œâ”€ Parse nested user object
â”‚     â”‚  â”œâ”€ Extract: name.firstname â†’ first_name, address.street â†’ address_street
â”‚     â”‚  â””â”€ INSERT or UPDATE in users table
â”‚     â”‚
â”‚     â””â”€ Result: ~60 users in users table
â”‚
â”œâ”€ 4. ANALYTICAL PROCESSING
â”‚  â”œâ”€ PostgreSQL creates views from normalized data:
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ vw_product_ratings:
â”‚  â”‚  â”‚  â””â”€ SELECT category, price_category, source, 
â”‚  â”‚  â”‚        COUNT(*) as product_count, AVG(rating_value) as avg_rating
â”‚  â”‚  â”‚     FROM products
â”‚  â”‚  â”‚     GROUP BY category, price_category, source
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ vw_cart_analysis:
â”‚  â”‚  â”‚  â””â”€ SELECT day_of_week, month, source,
â”‚  â”‚  â”‚        COUNT(DISTINCT cart_id) as cart_count, SUM(quantity) as total_items
â”‚  â”‚  â”‚     FROM cart_items
â”‚  â”‚  â”‚     GROUP BY day_of_week, month, source
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ vw_cart_products:
â”‚  â”‚     â””â”€ SELECT cart_id, userId, p.title, c.quantity, 
â”‚  â”‚           (p.price * c.quantity) as total_price
â”‚  â”‚        FROM cart_items c JOIN products p ON ...
â”‚  â”‚
â”‚  â””â”€ Views now contain aggregated analytics
â”‚
â”œâ”€ 5. VISUALIZATION (Apache Superset)
â”‚  â”œâ”€ User opens Superset at http://localhost:8088
â”‚  â”œâ”€ Connect Superset to PostgreSQL
â”‚  â”œâ”€ Create dashboards from views:
â”‚  â”‚  â”œâ”€ Dashboard 1: Product Analysis
â”‚  â”‚  â”‚  â”œâ”€ Chart: Average Rating by Category (vw_product_ratings)
â”‚  â”‚  â”‚  â”œâ”€ Chart: Price Distribution (vw_product_ratings)
â”‚  â”‚  â”‚  â””â”€ Chart: Source Comparison (vw_data_source_comparison)
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Dashboard 2: Shopping Patterns
â”‚  â”‚  â”‚  â”œâ”€ Chart: Carts by Day of Week (vw_cart_analysis)
â”‚  â”‚  â”‚  â”œâ”€ Chart: Items Sold by Month (vw_cart_analysis)
â”‚  â”‚  â”‚  â””â”€ Chart: Peak Shopping Times
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Dashboard 3: Multi-Source Comparison
â”‚  â”‚     â”œâ”€ Table: Record Counts by Source
â”‚  â”‚     â”œâ”€ Chart: Average Price Differences
â”‚  â”‚     â””â”€ Chart: Source Coverage
â”‚  â”‚
â”‚  â””â”€ Real-time dashboards update as new data arrives
â”‚
â””â”€ END: User has real-time insights into merged e-commerce data
```

---

## ğŸ—‚ï¸ Project Structure

```
data-pipeline/
â”œâ”€â”€ docker-compose.yml               # Orchestrates all services
â”œâ”€â”€ Dockerfile                       # Python 3.9 + Java environment
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # This file
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ init-db.sql                 # PostgreSQL schema & views
â”‚   â””â”€â”€ superset_config.py           # Superset configuration
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ multi_api_producer.py    # Data ingestion (Kafka producer)
â”‚       â”œâ”€â”€ process_users.py         # User data processor (Kafka consumer)
â”‚       â”œâ”€â”€ spark_products_processor.py    # Product data processor (Spark)
â”‚       â””â”€â”€ spark_carts_processor.py       # Cart data processor (Spark)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ postgres/                   # PostgreSQL data volume
â”‚   â””â”€â”€ (other data files)
â”‚
â””â”€â”€ logs/
    â””â”€â”€ (application logs)
```

---

## ğŸš€ Quick Start

### **Prerequisites**
- Docker and Docker Compose
- 8GB RAM minimum
- ~2GB disk space

### **Installation & Startup**

```bash
# 1. Navigate to project directory
cd data-pipeline

# 2. Build and start all services
docker-compose up -d

# 3. Wait for all services to be healthy (~30 seconds)
docker-compose ps

# 4. Run data ingestion
docker-compose exec -T spark-master python /scripts/python/multi_api_producer.py

# 5. Run stream processors (in separate terminals)
docker-compose exec -T spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \
  --master spark://spark-master:7077 \
  /scripts/python/spark_products_processor.py

docker-compose exec -T spark-master spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \
  --master spark://spark-master:7077 \
  /scripts/python/spark_carts_processor.py

docker exec -T spark-master python /scripts/python/process_users.py

# 6. Access Superset
# Open browser: http://localhost:8088
# Login: admin/admin
```

---

## ğŸ“Š Database & Queries

### **Querying Results**

```bash
# Connect to PostgreSQL
docker exec -it postgres psql -U postgres_user -d data_fakestore_db

# View normalized products
SELECT * FROM products LIMIT 10;

# Analyze product ratings
SELECT * FROM vw_product_ratings;

# Analyze shopping patterns
SELECT * FROM vw_cart_analysis;

# Compare data sources
SELECT * FROM vw_data_source_comparison;
```

### **Key Metrics Available**

1. **Product Analytics**
   - Average rating by category
   - Price distribution by source
   - Product count by price category

2. **Shopping Behavior**
   - Peak shopping days/months
   - Items purchased per cart
   - Cart value distribution

3. **Data Quality**
   - Record counts by source
   - Coverage comparison
   - Data freshness

---

## ğŸ” Security & Best Practices

1. **Credentials**: Database credentials hardcoded in config (for demo only)
   - âš ï¸ Change in production
   - Use environment variables or secrets manager

2. **Network**: All services on internal Docker network
   - âœ… External exposure only on necessary ports

3. **Data Validation**: 
   - âœ… ON CONFLICT handling prevents duplicates
   - âœ… Schema validation in Spark processors

4. **Error Handling**:
   - âœ… Retry logic in Kafka connections (10 attempts)
   - âœ… Exception handling in all processors
   - âœ… Rollback on database errors

---

## ğŸ› ï¸ Troubleshooting

### **Common Issues**

| Issue | Cause | Solution |
|-------|-------|----------|
| Kafka connection failed | Kafka not fully started | Wait 30s, check `docker-compose ps` |
| PostgreSQL not accessible | Database not initialized | Check `docker logs postgres` |
| Spark job fails | Missing dependencies | Verify `--packages` in spark-submit |
| No data in tables | Producer not run | Execute `multi_api_producer.py` first |
| Duplicate records | Missing deduplication | Check processor's `dropDuplicates()` call |

### **Debugging**

```bash
# Check service health
docker-compose ps

# View logs
docker logs kafka
docker logs postgres
docker logs superset

# Test Kafka connectivity
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server kafka:9092 \
  --topic fakestore-products \
  --from-beginning

# Test PostgreSQL
docker exec -it postgres psql -U postgres_user -d data_fakestore_db \
  -c "SELECT COUNT(*) FROM products;"
```

---

## ğŸ“ˆ Monitoring & Metrics

### **Key Performance Indicators**

- **Ingestion Rate**: Records/second from APIs
- **Processing Latency**: Time from Kafka to PostgreSQL
- **Data Freshness**: Time since last update
- **Error Rate**: Failed records percentage
- **Throughput**: Total records processed

### **Dashboards Available**

1. **Spark Master UI**: `http://localhost:8083`
   - Job status and progress
   - Executor metrics

2. **Superset**: `http://localhost:8088`
   - Real-time analytics
   - Custom visualizations

---

## ğŸ¯ Architecture Patterns Used

1. **Event-Driven Architecture**: Kafka decouples producers/consumers
2. **Lambda Architecture**: Real-time stream + batch processing
3. **Data Normalization**: Common schema from heterogeneous sources
4. **Microservices**: Independent processors for each data type
5. **CQRS**: Separate read (views) from write (tables)

---

## ğŸ“š Learning Outcomes

By studying this project, you'll learn:

âœ… **Event Streaming**: Apache Kafka fundamentals  
âœ… **Stream Processing**: Apache Spark Structured Streaming  
âœ… **Data Engineering**: ETL/ELT patterns  
âœ… **Data Normalization**: Handling heterogeneous schemas  
âœ… **Real-time Analytics**: Streaming to data warehouse  
âœ… **Docker Orchestration**: Multi-container deployments  
âœ… **SQL & Analytics**: Data warehousing concepts  
âœ… **Python Data Processing**: PySpark and psycopg2  

---

## ğŸ“ License

[Specify your license - MIT, Apache 2.0, etc.]

---

## ğŸ¤ Support

For issues or questions:
1. Check the Troubleshooting section
2. Review Docker logs: `docker logs <service_name>`
3. Verify all services are running: `docker-compose ps`
4. Check PostgreSQL directly for data: `docker exec postgres psql ...`

---

**Last Updated**: November 2024  
**Architecture**: Event-Driven Streaming Data Pipeline  
**Status**: Production Ready (with config adjustments)
